---
title: "Applied Bayesian Analyses in R"
subtitle: "Part2: prior probability distributions"
author: "Sven De Maeyer"
format: 
  revealjs:
    theme: [simple, My_theme.scss]
    width: 1422
    height: 805
    slide-number: true
self-contained: true
execute: 
  echo: false
  include: true
  output: true
code:
  code-copy: true
  code-line-numbers: true
code-annotations: hover
---

```{r}
library(here)
library(tidyverse)
library(brms)
library(bayesplot)
library(ggmcmc)
library(patchwork)
library(priorsense)

load(
  file = here("Presentations", "MarathonData.RData")
)

MarathonTimes_Mod2 <-
  readRDS(file = 
            here("Presentations",
              "Output",
              "MarathonTimes_Mod2.RDS")
          )

# Setting a plotting theme
theme_set(theme_linedraw() +
            theme(text = element_text(family = "Times", size = 10),
                  panel.grid = element_blank())
)
```

```{css echo=FALSE}
.small-code{
  font-size: 75%  
}
```

## When to Worry and How to Avoid Misuse of Bayesian Statistics {.smaller}

by Laurent Smeets and Rens van der Schoot

::: columns
::: {.column width="33%"}
Before estimating the model:

<b>1.  Do you understand the priors?</b>
:::

::: {.column width="33%"}
After estimation before inspecting results:

2.  Does the trace-plot exhibit convergence?
3.  Does convergence remain after doubling the number of iterations?
4.  Does the posterior distribution histogram have enough information?
5.  Do the chains exhibit a strong degree of autocorrelation?
6.  Do the posterior distributions make substantive sense?
:::

::: {.column width="33%"}
Understanding the exact influence of the priors

7.  Do different specification of the multivariate variance priors influence the results?
8.  Is there a notable effect of the prior when compared with non-informative priors?
9.  Are the results stable from a sensitivity analysis?
10. Is the Bayesian way of interpreting and reporting model results used?
:::
:::

::: aside
Tutorial source: <https://www.rensvandeschoot.com/brms-wambs/>
:::



# Focus on the [priors]{style="color: #447099"} before estimation

## Remember: priors come in many disguises

::: columns
::: {.column width="50%"}
[Uninformative/Weakly informative]{style="color: #447099"}

When objectivity is crucial and you want *let the data speak for itself...*
:::

::: {.column width="50%"}
[Informative]{style="color: #447099"}

When including significant information is crucial

-   previously collected data
-   results from former research/analyses
-   data of another source
-   theoretical considerations
-   elicitation
:::
:::

## What do we exactly mean by "informativeness"?


```{r, out.height = "50%", out.width="50%", echo = FALSE}
knitr::include_graphics("priors_vanderschoot.jpg")
```

[Van De Schoot, R., et al. (2021). Bayesian statistics and modelling. Nature Reviews Methods Primers, 1(1). https://doi.org/10.1038/s43586-020-00001-2]{style="font-size: smaller"}

## Uninformative priors can become very informative

-   For each parameter in the model we set priors

-   In a complex model there can be a complex interplay between priors

-   Setting weak priors for each single parameter may result in a lot of information...

-   Informativeness can only be judged in comparison to the likelihood

-   Prior predictive checking (see later) can help to see the informativeness on the scale of the outcome ==\> especially helpful for large models

Suggested help source: https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations

## `brms` defaults

-   Weakly informative priors

-   If dataset is big, impact of priors is minimal

-   But, always better to know what you are doing!

-   Complex models might run into convergence issues $\rightarrow$ specifying more informative priors might help!

So, how to deviate from the defaults?

## We have to think!! {background-color="#447099" transition="slide-in"}

Remember our model 2 for Marathon Times (see slides Part 1):

$$\begin{aligned}
& \text{MarathonTimeM}_i \sim N(\mu,\sigma_e)\\
& \mu = \beta_0 + \beta_1*\text{km4week}_i + \beta_2*\text{sp4week}_i 
\end{aligned}$$

What are priors for each of the parameters?

What do we need to be aware of to start thinking in priors?

Note: I centred both `km4week` and `sp4week` around their mean!

## Preparations for applying it to Marathon model

Packages needed:

```{r}
#| echo: true
#| eval: false

library(here)
library(tidyverse)
library(brms)
library(bayesplot)
library(ggmcmc)
library(patchwork)
library(priorsense)
```



## Preparations for applying it to Marathon model

Load the dataset and the model:

```{r}
#| echo: true
#| eval: false
load(
  file = here("Presentations", "MarathonData.RData")
)

MarathonTimes_Mod2 <-
  readRDS(file = 
            here("Presentations",
              "Output",
              "MarathonTimes_Mod2.RDS")
          )
```


## Check priors used by `brms`

Function: `get_prior( )`

Remember our model 2 for Marathon Times:

$$\begin{aligned}
& \text{MarathonTimeM}_i \sim N(\mu,\sigma_e)\\
& \mu = \beta_0 + \beta_1*\text{km4week}_i + \beta_2*\text{sp4week}_i 
\end{aligned}$$

```{r}
#| echo: true
#| eval: false

get_prior(
  MarathonTimeM ~ 1 + km4week + sp4week, 
  data = MarathonData
)
```

## Check priors used by `brms`

```{r, out.height = "70%", out.width="70%", echo = FALSE}
knitr::include_graphics("Priors_Mod2.jpg")
```

-   `prior`: type of prior distribution
-   `class`: parameter class (with `b` being population-effects)
-   `coef`: name of the coefficient within parameter class
-   `group`: grouping factor for group-level parameters (when using mixed effects models)
-   `resp` : name of the response variable when using multivariate models
-   `lb` & `ub`: lower and upper bound for parameter restriction

## Visualizing priors

The best way to make sense of the priors used is visualizing them!

Many options:

-   The Zoo of Distributions <https://ben18785.shinyapps.io/distribution-zoo/>
-   making your own visualizations

See WAMBS template!

There we demonstrate the use of `ggplot2`, `metRology`, `ggtext` and `patchwork` to visualize the priors.

## Visualizing priors

```{r}
#| message: false
#| warning: false
#| error: false
#| echo: true
#| output-location: slide
#| fig-cap: "Probability density plots for the different priors used in the example model"
#| fig-cap-location: margin

library(metRology)
library(ggtext)

# Setting a plotting theme

theme_set(theme_linedraw() +
            theme(text = element_text(family = "Times", size = 10),
                  panel.grid = element_blank(),
                  plot.title = element_markdown())
)

# Generate the plot for the prior of the Intercept (mu)
Prior_mu <- ggplot( ) +
  stat_function(
    fun = dt.scaled,    # We use the dt.scaled function of metRology
    args = list(df = 3, mean = 199.2, sd = 24.9), # 
    xlim = c(120,300)
  ) +
  scale_y_continuous(name = "density") +
  labs(title = "Prior for the intercept",
       subtitle = "student_t(3,199.2,24.9)")

# Generate the plot for the prior of the error variance (sigma)
Prior_sigma <- ggplot( ) +
  stat_function(
    fun = dt.scaled,    # We use the dt.scaled function of metRology
    args = list(df = 3, mean = 0, sd = 24.9), # 
    xlim = c(0,6)
  ) +
  scale_y_continuous(name = "density") +
  labs(title = "Prior for the residual variance",
       subtitle = "student_t(3,0,24.9)")

# Generate the plot for the prior of the effects of independent variables
Prior_betas <- ggplot( ) +
  stat_function(
    fun = dnorm,    # We use the normal distribution
    args = list(mean = 0, sd = 10), # 
    xlim = c(-20,20)
  ) +
  scale_y_continuous(name = "density") +
  labs(title = "Prior for the effects of independent variables",
       subtitle = "N(0,10)")

Prior_mu + Prior_sigma + Prior_betas +
  plot_layout(ncol = 3)
```

## Understanding priors... Another example {background-color="#447099" transition="slide-in"}

Experimental study (pretest - posttest design) with 3 conditions:

-   control group;
-   experimental group 1;
-   experimental group 2.

Model:

$$\begin{aligned}
  & Posttest_{i}  \sim N(\mu,\sigma_{e_{i}})\\
  & \mu = \beta_0 + \beta_1*\text{Pretest}_{i} + \beta_2*\text{Exp_cond1}_{i} + \beta_3*\text{Exp_cond2}_{i}
\end{aligned}$$

Our job: coming up with priors that reflect that we expect both conditions to have a positive effect (hypothesis based on literature) and no indications that one experimental condition will outperform the other.

## Understanding priors... Another example

-   Assuming pre- and posttest are standardized
-   Assuming no increase between pre- and posttest in control condition

```{r}
# Generate the plot for the prior of the Intercept (mu)
Prior_mu <- ggplot( ) +
  stat_function(
    fun = dnorm,    # We use a normal distribution
    args = list(mean = 0, sd = .5), # 
    xlim = c(-1,1)
  ) +
  scale_y_continuous(name = "density") +
  labs(title = "Prior for the intercept",
       subtitle = "N(0,0.5)")

Prior_mu
```

## Understanding priors... Another example

-   Assuming a strong correlation between pre- and posttest

```{r}
# Generate the plot for the prior of beta 1
Prior_beta1 <- ggplot( ) +
  stat_function(
    fun = dnorm,    # We use the dt.scaled function of metRology
    args = list(mean = 1, sd = 0.5), # 
    xlim = c(-1,3)
  ) +
  scale_y_continuous(name = "density") +
  labs(title = "Prior for the effect of pretest score",
       subtitle = "N(1,0.5)")

Prior_beta1
```

## Understanding priors... Another example

-   Assuming a small effect of experimental conditions
-   No difference between both experimental conditions

```{r}
# Generate the plot for the prior of the effects experimental conditions
Prior_betas <- ggplot( ) +
  stat_function(
    fun = dnorm,    # We use the normal distribution
    args = list(mean = 0.2, sd = .6), # 
    xlim = c(-1,2)
  ) +
  scale_y_continuous(name = "density") +
  labs(title = "Prior for the effects of experimental conditions",
       subtitle = "N(0.2,0.6)")

Prior_betas
```

Remember Cohen's d: 0.2 = small effect size; 0.5 = medium effect size; 0.8 or higher = large effect size

## `r fontawesome::fa("laptop-code", "white")` Your Turn {background-color="#447099" transition="slide-in"}

-   Your data and model
-   What are the priors set by `brms`?
-   Can you come up with custom priors for certain parameters?
-   Try to build a rationale/argumentation for them
-   Visualize the custom or default priors

*DO NOT HESITATE TO ASK FOR GUIDANCE HERE*

::: callout-tip
## Tip

Consider re-scaling your (in)dependent variables if it is hard to make sense of parameters a priori. E.g., standardizing variables enables you to think in effect sizes.
:::

## Setting custom priors in `brms`

<br>

Setting our custom priors can be done with `set_prior( )` command

<br>

E.g., change the priors for the beta's (effects of `km4week` and `sp4week`):

<br>

```{r}
#| message: false
#| warning: false
#| error: false
#| echo: true
#| cache: true


Custom_priors <- 
  c(
    set_prior(
      "normal(0,10)", 
      class = "b", 
      coef = "km4week"),
    set_prior(
      "normal(0,10)", 
      class = "b", 
      coef = "sp4week")
    )

```

## Prior Predictive Check

<br>

Did you set sensible priors?

<br>

-   Simulate data based on the model and the priors

<br>

-   Visualize the simulated data and compare with real data

<br>

-   Check if the plot shows impossible simulated datasets

## Prior Predictive Check in `brms`

<br>

Step 1: Fit the model with custom priors with option `sample_prior="only"`

<br>

```{r}
#| message: false
#| warning: false
#| error: false
#| echo: true
#| eval: false
#| cache: true
#| code-line-numbers: "5|8"
Fit_Model_priors <- 
  brm(
    MarathonTimeM ~ 1 + km4week + sp4week, 
    data = MarathonData,
    prior = Custom_priors,
    backend = "cmdstanr",
    cores = 4,
    sample_prior = "only"
    )
```

```{r}
Fit_Model_priors <- readRDS(
  here(
    "Presentations",
    "Output",
    "Fit_Model_priors.RDS"
  )
)
```

## Prior Predictive Check in `brms`

<br>

Step 2: visualize the data with the `pp_check( )` function

<br>

```{r}
#| echo: true
#| output-location: slide

set.seed(1975)

pp_check(
  Fit_Model_priors, 
  ndraws = 300) # number of simulated datasets you wish for

```

## Check some summary statistics

-   How are summary statistics of simulated datasets (e.g., median, min, max, ...) distributed over the datasets?

-   How does that compare to our real data?

-   Use `type = "stat"` argument within `pp_check()`

```{r}
#| echo: true
#| output-location: slide
pp_check(Fit_Model_priors, 
         type = "stat", 
         stat = "median")
```

## `r fontawesome::fa("laptop-code", "white")` Your Turn {background-color="#447099" transition="slide-in"}

-   Your data and model

-   Perform a prior predictive check

-   If necessary re-think your priors and check again

# Prior sensitivity analyses

## Why prior sensitivity analyses?

-   Often we rely on 'arbitrary' chosen (default) weakly informative priors

-   What is the influence of the prior (and the likelihood) on our results?

-   You could ad hoc set new priors and re-run the analyses and compare (a lot of work, without strict sytematical guidelines)

-   Semi-automated checks can be done with `priorsense` package

## Using the `priorsense` package

Recently a package dedicated to prior sensitivity analyses is launched

```{r}
#| eval: false
#| echo: true
# install.packages("remotes")
remotes::install_github("n-kall/priorsense")
```

Key-idea: power-scaling (both prior and likelihood)

background reading:

-   <https://arxiv.org/pdf/2107.14054.pdf>

YouTube talk:

-   <https://www.youtube.com/watch?v=TBXD3HjcIps&t=920s>

## Basic table with indices

First check is done by using the `powerscale_sensitivity( )` function

-   column prior contains info on sensitivity for prior (should be lower than 0.05)

-   column likelihood contains info on sensitivity for likelihood (that we want to be high, 'let our data speak')

-   column diagnosis is a verbalization of potential problem (- if none)

```{r}
#| echo: true
#| output-location: slide
powerscale_sensitivity(MarathonTimes_Mod2)
```

## Visualization of prior sensitivity

```{r}
#| echo: true
#| warning: false
#| message: false
#| cache: true
#| output-location: slide

powerscale_plot_dens(
  powerscale_sequence(
    MarathonTimes_Mod2
    ),
  variables = c(
      "b_Intercept",
      "b_km4week",
      "b_sp4week"
    )
  )
```

## Visualization of prior sensitivity

```{r}
#| echo: true
#| warning: false
#| message: false
#| cache: true
#| output-location: slide

powerscale_plot_quantities(
  powerscale_sequence(
    MarathonTimes_Mod2
    ),
  variables = c(
      "b_km4week"
      )
  )
```

## `r fontawesome::fa("laptop-code", "white")` Your Turn {background-color="#447099" transition="slide-in"}

-   Your data and model

-   Check the prior sensitivity of your results


## WAMBS Template to use

-   File called [WAMBS_workflow_MarathonData.qmd]{style="color: #447099"} (quarto document)

-   <a href="https://abar-geneva-2024.netlify.app/WAMBS_template/WAMBS_workflow_MarathonData.qmd" target="blank">Click here </a> for the Quarto version

-   Create your own project and project folder

-   Copy the template and rename it

-   You can apply/adapt the code in the template

-   To render the document properly with references, you also need the <a href="https://abar-geneva-2024.netlify.app/WAMBS_template/references.bib" target="blank">references.bib file </a>

## Side-path: projects in RStudio and the `here` package

If you do not know how to use Projects in RStudio or the `here` package, these two sources might be helpfull:

Projects: <https://youtu.be/MdTtTN8PUqU?si=mmQGlU063EMt86B2>

`here` package: <https://youtu.be/oh3b3k5uM7E?si=0-heLJXfFVLtTohh>
