---
title: "Applied Bayesian Analyses in R"
subtitle: "Part2: prior probability distributions"
author: "Sven De Maeyer"
format: 
  revealjs:
    theme: [simple, My_theme.scss]
    width: 1422
    height: 805
    slide-number: true
self-contained: true
execute: 
  echo: false
  include: true
  output: true
code:
  code-copy: true
  code-line-numbers: true
code-annotations: hover
---

```{r}
library(here)
library(tidyverse)
library(brms)
library(bayesplot)
library(ggmcmc)
library(patchwork)
library(priorsense)

load(
  file = here("Presentations", "MarathonData.RData")
)

MarathonTimes_Mod2 <-
  readRDS(file = 
            here("Presentations",
              "Output",
              "MarathonTimes_Mod2.RDS")
          )

# Setting a plotting theme
theme_set(theme_linedraw() +
            theme(text = element_text(family = "Times", size = 10),
                  panel.grid = element_blank())
)
```

## `r fontawesome::fa("laptop-code", "white")` Your Turn {background-color="#447099" transition="slide-in"}

-   Open `MarathonData.RData`
-   Estimate two Bayesian Models
-   Model1: only an intercept
-   Model2: introduce the effect of `km4week` and `sp4week`on `MarathonTimeM`
-   Make plots with the `plot()` function
-   What do we learn?

::: aside
[Note: I centred both]{style="color: white"} `km4week`[and]{style="color: white"} `sp4week` [around their mean!]{style="color: white"}
:::

##  {background-color="#447099" transition="slide-in"}

```{r}
#| echo: true
#| eval: false

MarathonTimes_Mod2 <- brm(                        
  MarathonTimeM ~ km4week + sp4week, 
  data = MarathonData,                         
  backend = "cmdstanr",  
  cores = 4,
  seed = 1975                          
)
```

## Model 

```{r}
#| eval: true
#| echo: true
#| output-location: slide

```

# WAMBS checklist

```{css echo=FALSE}
.small-code{
  font-size: 75%  
}
```

## When to Worry and How to Avoid Misuse of Bayesian Statistics {.smaller}

by Laurent Smeets and Rens van der Schoot

::: columns
::: {.column width="33%"}
Before estimating the model:

1.  Do you understand the priors?
:::

::: {.column width="33%"}
After estimation before inspecting results:

2.  Does the trace-plot exhibit convergence?
3.  Does convergence remain after doubling the number of iterations?
4.  Does the posterior distribution histogram have enough information?
5.  Do the chains exhibit a strong degree of autocorrelation?
6.  Do the posterior distributions make substantive sense?
:::

::: {.column width="33%"}
Understanding the exact influence of the priors

7.  Do different specification of the multivariate variance priors influence the results?
8.  Is there a notable effect of the prior when compared with non-informative priors?
9.  Are the results stable from a sensitivity analysis?
10. Is the Bayesian way of interpreting and reporting model results used?
:::
:::

::: aside
Tutorial source: <https://www.rensvandeschoot.com/brms-wambs/>
:::

## WAMBS Template to use

-   File called [WAMBS_workflow_MarathonData.qmd]{style="color: #447099"} (quarto document)

-   <a href="https://abar-geneva-2024.netlify.app/WAMBS_template/WAMBS_workflow_MarathonData.qmd" target="blank">Click here </a> for the Quarto version

-   Create your own project and project folder

-   Copy the template and rename it

-   You can apply/adapt the code in the template

-  To render the document properly with references, you also need the <a href="https://abar-geneva-2024.netlify.app/WAMBS_template/references.bib" target="blank">references.bib file </a>

## Side-path: projects in RStudio and the `here` package

If you do not know how to use Projects in RStudio or the `here` package, these two sources might be helpfull:

Projects: <https://youtu.be/MdTtTN8PUqU?si=mmQGlU063EMt86B2>

`here` package: <https://youtu.be/oh3b3k5uM7E?si=0-heLJXfFVLtTohh>

## Preparations for applying it to Marathon model

Packages needed:

```{r}
#| echo: true
#| eval: false

library(here)
library(tidyverse)
library(brms)
library(bayesplot)
library(ggmcmc)
library(patchwork)
library(priorsense)
```

## Preparations for applying it to Marathon model

Load the dataset and the model:

```{r}
#| echo: true
#| eval: false
load(
  file = here("Presentations", "MarathonData.RData")
)

MarathonTimes_Mod2 <-
  readRDS(file = 
            here("Presentations",
              "Output",
              "MarathonTimes_Mod2.RDS")
          )
```

# Focus on the [priors]{style="color: #447099"} before estimation

## Remember: priors come in many disguises

::: columns
::: {.column width="50%"}
[Uninformative/Weakly informative]{style="color: #447099"}

When objectivity is crucial and you want *let the data speak for itself...*
:::

::: {.column width="50%"}
[Informative]{style="color: #447099"}

When including significant information is crucial

-   previously collected data
-   results from former research/analyses
-   data of another source
-   theoretical considerations
-   elicitation
:::
:::

## What do we exactly mean by "informativeness"?

```{r, out.height = "99%", out.width="99%", echo = FALSE}
knitr::include_graphics("priors_vanderschoot.jpg")
```

Van De Schoot, R., Depaoli, S., King, R., Kramer, B., MÃ¤rtens, K., Tadesse, M. G., Vannucci, M., Gelman, A., Veen, D., Willemsen, J., & Yau, C. (2021). Bayesian statistics and modelling. Nature Reviews Methods Primers, 1(1). https://doi.org/10.1038/s43586-020-00001-2


## Uninformative priors can become very informative

- For each parameter in the model we set priors

- In a complex model there can be a complex interplay between priors

- Setting weak priors for each single parameter may result in a lot of information...

- Informativeness can only be judged in comparison to the likelihood

- Prior predictive checking (see later) can help to see the informativeness on the scale of the outcome ==> especially helpful for large models

Suggested help source:
https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations


## `brms` defaults

-   Weakly informative priors

-   If dataset is big, impact of priors is minimal

-   But, always better to know what you are doing!

-   Complex models might run into convergence issues $\rightarrow$ specifying more informative priors might help!

So, how to deviate from the defaults?

## Check priors used by `brms`

Function: `get_prior( )`

Remember our model 2 for Marathon Times:

$$\begin{aligned}
& \text{MarathonTimeM}_i \sim N(\mu,\sigma_e)\\
& \mu = \beta_0 + \beta_1*\text{km4week}_i + \beta_2*\text{sp4week}_i 
\end{aligned}$$

```{r}
#| echo: true
#| eval: false

get_prior(
  MarathonTimeM ~ 1 + km4week + sp4week, 
  data = MarathonData
)
```

## Check priors used by `brms`

```{r, out.height = "70%", out.width="70%", echo = FALSE}
knitr::include_graphics("Priors_Mod2.jpg")
```

-   `prior`: type of prior distribution
-   `class`: parameter class (with `b` being population-effects)
-   `coef`: name of the coefficient within parameter class
-   `group`: grouping factor for group-level parameters (when using mixed effects models)
-   `resp` : name of the response variable when using multivariate models
-   `lb` & `ub`: lower and upper bound for parameter restriction

## Visualizing priors

The best way to make sense of the priors used is visualizing them!

Many options:

-   The Zoo of Distributions <https://ben18785.shinyapps.io/distribution-zoo/>
-   making your own visualizations

See WAMBS template!

There we demonstrate the use of `ggplot2`, `metRology`, `ggtext` and `patchwork` to visualize the priors.

## Visualizing priors

```{r}
#| message: false
#| warning: false
#| error: false
#| echo: true
#| output-location: slide
#| fig-cap: "Probability density plots for the different priors used in the example model"
#| fig-cap-location: margin
#| cache: true

library(metRology)
library(ggplot2)
library(ggtext)
library(patchwork)

# Setting a plotting theme
theme_set(theme_linedraw() +
            theme(text = element_text(family = "Times", size = 10),
                  panel.grid = element_blank(),
                  plot.title = element_markdown())
)

# Generate the plot for the prior of the Intercept (mu)
Prior_mu <- ggplot( ) +
  stat_function(
    fun = dt.scaled,    # We use the dt.scaled function of metRology
    args = list(df = 3, mean = 199.2, sd = 24.9), # 
    xlim = c(120,300)
  ) +
  scale_y_continuous(name = "density") +
  labs(title = "Prior for the intercept",
       subtitle = "student_t(3,199.2,24.9)")

# Generate the plot for the prior of the error variance (sigma)
Prior_sigma <- ggplot( ) +
  stat_function(
    fun = dt.scaled,    # We use the dt.scaled function of metRology
    args = list(df = 3, mean = 0, sd = 24.9), # 
    xlim = c(0,6)
  ) +
  scale_y_continuous(name = "density") +
  labs(title = "Prior for the residual variance",
       subtitle = "student_t(3,0,24.9)")

# Generate the plot for the prior of the effects of independent variables
Prior_betas <- ggplot( ) +
  stat_function(
    fun = dnorm,    # We use the normal distribution
    args = list(mean = 0, sd = 10), # 
    xlim = c(-20,20)
  ) +
  scale_y_continuous(name = "density") +
  labs(title = "Prior for the effects of independent variables",
       subtitle = "N(0,10)")

Prior_mu + Prior_sigma + Prior_betas +
  plot_layout(ncol = 3)
```

## Understanding priors... Another example

Experimental study (pretest - posttest design) with 3 conditions:

- control group;
- experimental group 1;
- experimental group 2.

Model:

$$\begin{aligned}
  & Posttest_{i}  \sim N(\mu,\sigma_{e_{i}})\\
  & \mu = \beta_0 + \beta_1*\text{Pretest}_{i} + \beta_2*\text{Exp_cond1}_{i} + \beta_3*\text{Exp_cond2}_{i}
\end{aligned}$$

Our job: coming up with priors that reflect that we expect both conditions to have a positive effect (hypothesis based on literature) and no indications that one experimental condition will outperform the other.

## Understanding priors... Another example

- Assuming pre- and posttest are standardized
- Assuming no increase between pre- and posttest in control condition

```{r}
# Generate the plot for the prior of the Intercept (mu)
Prior_mu <- ggplot( ) +
  stat_function(
    fun = dnorm,    # We use a normal distribution
    args = list(mean = 0, sd = .5), # 
    xlim = c(-1,1)
  ) +
  scale_y_continuous(name = "density") +
  labs(title = "Prior for the intercept",
       subtitle = "N(0,0.5)")

Prior_mu
```

## Understanding priors... Another example

- Assuming a strong correlation between pre- and posttest

```{r}
# Generate the plot for the prior of beta 1
Prior_beta1 <- ggplot( ) +
  stat_function(
    fun = dnorm,    # We use the dt.scaled function of metRology
    args = list(mean = 1, sd = 0.5), # 
    xlim = c(-1,3)
  ) +
  scale_y_continuous(name = "density") +
  labs(title = "Prior for the effect of pretest score",
       subtitle = "N(1,0.5)")

Prior_beta1
```

## Understanding priors... Another example

- Assuming a small effect of experimental conditions
- No difference between both experimental conditions

```{r}
# Generate the plot for the prior of the effects experimental conditions
Prior_betas <- ggplot( ) +
  stat_function(
    fun = dnorm,    # We use the normal distribution
    args = list(mean = 0.2, sd = .6), # 
    xlim = c(-1,2)
  ) +
  scale_y_continuous(name = "density") +
  labs(title = "Prior for the effects of experimental conditions",
       subtitle = "N(0.2,0.6)")

Prior_betas
```

Remember Cohen's d: 0.2 = small effect size; 0.5 = medium effect size; 0.8 or higher = large effect size

## `r fontawesome::fa("laptop-code", "white")` Your Turn {background-color="#447099" transition="slide-in"}

-   Your data and model
-   What are the priors set by `brms`?
-   Can you come up with custom priors for certain parameters?
-   Try to build a rationale/argumentation for them
-   Visualize the custom or default priors

*DO NOT HESITATE TO ASK FOR GUIDANCE HERE*

::: callout-tip
## Tip

Consider re-scaling your (in)dependent variables if it is hard to make sense of parameters a priori. E.g., standardizing variables enables you to think in effect sizes.
:::

## Setting custom priors in `brms`

<br>

Setting our custom priors can be done with `set_prior( )` command

<br>

E.g., change the priors for the beta's (effects of `km4week` and `sp4week`):

<br>

```{r}
#| message: false
#| warning: false
#| error: false
#| echo: true
#| cache: true


Custom_priors <- 
  c(
    set_prior(
      "normal(0,10)", 
      class = "b", 
      coef = "km4week"),
    set_prior(
      "normal(0,10)", 
      class = "b", 
      coef = "sp4week")
    )

```

## Prior Predictive Check

<br>

Did you set sensible priors?

<br>

-   Simulate data based on the model and the priors

<br>

-   Visualize the simulated data and compare with real data

<br>

-   Check if the plot shows impossible simulated datasets

## Prior Predictive Check in `brms`

<br>

Step 1: Fit the model with custom priors with option `sample_prior="only"`

<br>

```{r}
#| message: false
#| warning: false
#| error: false
#| echo: true
#| eval: false
#| cache: true
#| code-line-numbers: "5|8"
Fit_Model_priors <- 
  brm(
    MarathonTimeM ~ 1 + km4week + sp4week, 
    data = MarathonData,
    prior = Custom_priors,
    backend = "cmdstanr",
    cores = 4,
    sample_prior = "only"
    )
```

```{r}
Fit_Model_priors <- readRDS(
  here(
    "Presentations",
    "Output",
    "Fit_Model_priors.RDS"
  )
)
```

## Prior Predictive Check in `brms`

<br>

Step 2: visualize the data with the `pp_check( )` function

<br>

```{r}
#| echo: true
#| output-location: slide

set.seed(1975)

pp_check(
  Fit_Model_priors, 
  ndraws = 300) # number of simulated datasets you wish for

```

## Check some summary statistics

-   How are summary statistics of simulated datasets (e.g., median, min, max, ...) distributed over the datasets?

-   How does that compare to our real data?

-   Use `type = "stat"` argument within `pp_check()`

```{r}
#| echo: true
#| output-location: slide
pp_check(Fit_Model_priors, 
         type = "stat", 
         stat = "median")
```

## `r fontawesome::fa("laptop-code", "white")` Your Turn {background-color="#447099" transition="slide-in"}

-   Your data and model

-   Perform a prior predictive check

-   If necessary re-think your priors and check again

# Focus on convergence of the model (before interpreting the model!)

## Does the trace-plot exhibit convergence?

<br>

Create custom trace-plots (aka caterpillar plots) with `ggs( )` function from `ggmcmc` package

```{r}
#| fig-cap: "Caterpillar plots for the parameters in the model"
#| fig-cap-location: margin
#| output-location: slide
#| cache: true
#| echo: true
#| warning: false
Model_chains <- ggs(MarathonTimes_Mod2)

Model_chains %>%
  filter(Parameter %in% c(
          "b_Intercept", 
          "b_km4week", 
          "b_sp4week", 
          "sigma"
          )
  ) %>%
  ggplot(aes(
    x   = Iteration,
    y   = value, 
    col = as.factor(Chain)))+
  geom_line() +
  facet_grid(Parameter ~ .,
             scale  = 'free_y',
             switch = 'y') +
  labs(title = "Caterpillar Plots for the parameters",
       col   = "Chains")
```

## Does convergence remain after doubling the number of iterations?

<br>

Re-fit the model with more iterations

<br>

Check trace-plots again

<br>

::: callout-warning
First consider the need to do this! If you have a complex model that already took a long time to run, this check will take at least twice as much time...
:::

## `r fontawesome::fa("laptop-code", "white")` Your Turn {background-color="#447099" transition="slide-in"}

-   Your data and model
-   Do the first checks on the model convergence

## R-hat statistics

Sampling of parameters done by:

-   multiple chains
-   multiple iterations within chains

If variance between chains is big $\rightarrow$ NO CONVERGENCE

R-hat ($\widehat{R}$) : compares the between- and within-chain estimates for model parameters

## R-hat statistics

::: columns
::: {.column width="50%"}
```{r, out.height = "95%", out.width="95%", echo = FALSE}
knitr::include_graphics("Vethari_paper.jpg")
```
:::

::: {.column width="50%"}
-   $\widehat{R}$ \< 1.05 for each parameter estimate

-   at least 4 chains are recommended

-   Effective Sample Size (ESS) \> 400 to rely on $\widehat{R}$
:::
:::

## R-hat in `brms`

`mcmc_rhat()` function from the `bayesplot` package

```{r}
#| echo: true
#| output-location: slide

mcmc_rhat(rhat(MarathonTimes_Mod2), 
          size = 3
          )+ 
  yaxis_text(hjust = 1)  # to print parameter names
```

## `r fontawesome::fa("laptop-code", "white")` Your Turn {background-color="#447099" transition="slide-in"}

-   Your data and model

-   Check the R-hat statistics

## Autocorrelation

-   Sampling of parameter values are not independent!

-   So there is autocorrelation

-   But you don't want too much impact of autocorrelation

-   2 approaches to check this:

    -   ratio of the effective sample size to the total sample size
    -   plot degree of autocorrelation

## Ratio effective sample size / total sample size

-   Should be higher than 0.1 (Gelman et al., 2013)

-   Visualize making use of the `mcmc_neff( )` function from `bayesplot`

```{r}
#| echo: true
#| output-location: slide
mcmc_neff(
  neff_ratio(MarathonTimes_Mod2)
  ) + 
  yaxis_text(hjust = 1)  # to print parameter names
```

## Plot degree of autocorrelation

-   Visualize making use of the `mcmc_acf( )` function

```{r}
#| echo: true
#| output-location: slide
mcmc_acf(
  as.array(MarathonTimes_Mod2), 
  regex = "b") # to plot only the parameters starting with b (our beta's)
```

## `r fontawesome::fa("laptop-code", "white")` Your Turn {background-color="#447099" transition="slide-in"}

-   Your data and model

-   Check the autocorrelation

## Rank order plots

-   additional way to assess the convergence of MCMC

-   if the algorithm converged, plots of all chains look similar

```{r}
#| echo: true
#| output-location: slide

mcmc_rank_hist(
  MarathonTimes_Mod2, 
  regex = "b" # only intercept and beta's
  ) 
```

## `r fontawesome::fa("laptop-code", "white")` Your Turn {background-color="#447099" transition="slide-in"}

-   Your data and model

-   Check the rank order plots

# Focus on the Posterior

## Does the posterior distribution histogram have enough information?

-   Histogram of posterior for each parameter

-   Have clear peak and sliding slopes

## Plotting the posterior distribution histogram

<br>

Step 1: create a new object with 'draws' based on the final model

<br>

```{r}
#| echo: true
posterior_PD <- as_draws_df(MarathonTimes_Mod2)
```

## Plotting the posterior distribution histogram

<br>

Step 2: create histogram making use of that object

<br>

```{r}
#| echo: true

post_intercept <- 
  posterior_PD %>%
  select(b_Intercept) %>%
  ggplot(aes(x = b_Intercept)) +
  geom_histogram() +
  ggtitle("Intercept") 

post_km4week <- 
  posterior_PD %>%
  select(b_km4week) %>%
  ggplot(aes(x = b_km4week)) +
  geom_histogram() +
  ggtitle("Beta km4week") 

post_sp4week <- 
  posterior_PD %>%
  select(b_sp4week) %>%
  ggplot(aes(x = b_sp4week)) +
  geom_histogram() +
  ggtitle("Beta sp4week") 
```

## Plotting the posterior distribution histogram

<br>

Step 3: print the plot making use of `patchwork` 's workflow to combine plots <br>

```{r}
#| echo: true
#| output-location: slide
post_intercept + post_km4week + post_sp4week +
  plot_layout(ncol = 3)
```

## Posterior Predictive Check

-   Generate data based on the posterior probability distribution

-   Create plot of distribution of y-values in these simulated datasets

-   Overlay with distribution of observed data

using `pp_check()` again, now with our model

```{r}
#| echo: true
#| output-location: slide
pp_check(MarathonTimes_Mod2, 
         ndraws = 100)
```

## Posterior Predictive Check

-   We can also focus on some summary statistics (like we did with prior predictive checks as well)

```{r}
#| echo: true
#| message: false
#| warning: false
#| output-location: slide
pp_check(MarathonTimes_Mod2, 
         ndraws = 300,
         type = "stat",
         stat = "median")
```

## `r fontawesome::fa("laptop-code", "white")` Your Turn {background-color="#447099" transition="slide-in"}

-   Your data and model

-   Focus on the posterior and do some checks!

# Prior sensibility analyses

## Why prior sensibility analyses?

-   Often we rely on 'arbitrary' chosen (default) weakly informative priors

-   What is the influence of the prior (and the likelihood) on our results?

-   You could ad hoc set new priors and re-run the analyses and compare (a lot of work, without strict sytematical guidelines)

-   Semi-automated checks can be done with `priorsense` package

## Using the `priorsense` package

Recently a package dedicated to prior sensibility analyses is launched

```{r}
#| eval: false
#| echo: true
# install.packages("remotes")
remotes::install_github("n-kall/priorsense")
```

Key-idea: power-scaling (both prior and likelihood)

background reading:

-   <https://arxiv.org/pdf/2107.14054.pdf>

YouTube talk:

-   <https://www.youtube.com/watch?v=TBXD3HjcIps&t=920s>

## Basic table with indices

First check is done by using the `powerscale_sensitivity( )` function

-   column prior contains info on sensibility for prior (should be lower than 0.05)

-   column likelihood contains info on sensibility for likelihood (that we want to be high, 'let our data speak')

-   column diagnosis is a verbalization of potential problem (- if none)

```{r}
#| echo: true
#| output-location: slide
powerscale_sensitivity(MarathonTimes_Mod2)
```

## Visualization of prior sensibility

```{r}
#| echo: true
#| warning: false
#| message: false
#| cache: true
#| output-location: slide

powerscale_plot_dens(
  powerscale_sequence(
    MarathonTimes_Mod2
    ),
  variables = c(
      "b_Intercept",
      "b_km4week",
      "b_sp4week"
    )
  )
```

## Visualization of prior sensibility

```{r}
#| echo: true
#| warning: false
#| message: false
#| cache: true
#| output-location: slide

powerscale_plot_quantities(
  powerscale_sequence(
    MarathonTimes_Mod2
    ),
  variables = c(
      "b_km4week"
      )
  )
```

## `r fontawesome::fa("laptop-code", "white")` Your Turn {background-color="#447099" transition="slide-in"}

-   Your data and model

-   Check the prior sensibility of your results
