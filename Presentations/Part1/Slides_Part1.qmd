---
title: "Applied Bayesian Analyses in R"
subtitle: "Part1: introduction"
author: "Sven De Maeyer"
format:
  revealjs: 
    theme: [simple, My_theme.scss]
    width: 1422
    height: 805
    slide-number: true
    include-in-header: 
      text: |
        <style>
        .center-xy {
          margin: 0;
          position: absolute;
          top: 50%;
          -ms-transform: translateY(-50%);
          transform: translateY(-50%);
        }
        </style>
editor: visual
self-contained: true
execute: 
  echo: false
  include: true
  output: true
code:
  code-copy: true
  code-line-numbers: true
code-annotations: hover
---

# Introduction

```{css echo=FALSE}
.small-code{
  font-size: 75%  
}
```

## Welcome

Let's get to know each other first!

-   what's your name?
-   a 1/2-minute pitch on your research
-   any experience with Bayesian inference?
-   what do you hope to learn?

## Outline

Part 1: Introduction to Bayesian inferences and some basics in `brms`

Part 2: Think and set priors in a simple model

Part 3: Comparing models and into Bayesian Mixed Effects Modesl

Part 4: Reporting and interpreting Bayesian models

Integrated exercise: Analysing your own data or integrated exercise when you want as homework

## Practical stuff

All material is on the on-line dashboard:

https://abar-turku2024.netlify.app/#quick-overview

You can find:

-   an overview of the course

-   how to prepare

-   for each day the slides and datasets

-   html slides can be exported to pdf in your browser (use the menu button bottom right)

# Statistical Inference

## Why statistical inference?

::: center-xy
We want to learn more about [a population]{style="color: #447099"} but we have [sample data]{style="color: #447099"}!

Therefore, we have [uncertainty]{style="color: red"}
:::

## Why statistical inference? an example

> What is the effect of average number of kilometres ran per week on the marathon time?

We can estimate the effect by using a regression model based on a sample:

$$\text{MarathonTime}_i = \beta_0 + \beta_1 * \text{n_km}_i + \epsilon_i$$

Of course, we are not interested in the value of $\beta_1$ in the sample, but we want to learn more on $\beta_1$ in the population!

## Frequentistic statistical inference (with data...)

Maximal Likelihood estimation:

::: small-code
```{r}
## Libraries

library(here)
library(tidyverse)
library(brms)
library(bayesplot)

## Load the data

MarathonData <- read_csv(
  file = here("Presentations", "MarathonData.csv")
)

MarathonData <- MarathonData |>
  mutate(
    MarathonTimeM = MarathonTime * 60,
    km4week = km4week - mean(km4week, na.rm = T)
  )

Model1 <- lm(MarathonTimeM ~ km4week, data = MarathonData)

summary(Model1)
```
:::

::: aside
Data comes from <https://www.kaggle.com/datasets/girardi69/marathon-time-predictions?resource=download>
:::

## Frequentistic statistical inference (interpretation ...)

```{r}
#| echo: false
lower <- -0.509 - (1.96*.07)
higher <- -0.509 + (1.96*.07)
```

Sample: for each km more a week approx half a minute faster

Population:

-   p-value = prob to observe a \|0.509\| estimate if effect is zero in the population is lower than 0.05
-   CI = 95% chance that the true population slope is a value between `r lower` and `r higher`

==\> So: -0.6 is as equally probable as -0.4 ...

## Bayesian statistical inference

```{r}
Model1_running <- readRDS(
  here("Presentations", "Output", "Model1_running.RDS")
)


mcmc_areas(
  Model1_running,
  pars = c("b_km4week"),
  prob = 0.89
) + theme_minimal()
```

## Advantages & Disadvantages of Bayesian analyses

::: columns
::: {.column width="50%"}
Advantages:

-   Natural approach to express uncertainty
-   Ability to incorporate prior knowledge
-   Increased model flexibility
-   Full posterior distribution of the parameters
-   Natural propagation of uncertainty
:::

::: {.column width="50%"}
Disadvantage:

-   Slow speed of model estimation
-   Some reviewers don't understand you (<i>"give me the p-value"</i>)
:::
:::

::: aside
Slight adaptation of a slide from Paul BÃ¼rkner's presentation available on YouTube: <https://www.youtube.com/watch?v=FRs1iribZME>
:::

# Bayesian Inference

## Bayesian Theorem

::: columns
::: {.column width="50%"}
```{r, out.height = "50%", out.width="50%", echo = FALSE}
knitr::include_graphics("prior_data_posterior.png")
```
:::

::: {.column width="50%"}
$$
P(\theta|data) = \frac{P(data|\theta)P(\theta)}{P(data)}
$$ with

-   $P(data|\theta)$ : the [likelihood]{style="color: #447099"} of the data given the parameters in the model $\theta$
-   $P(\theta)$ : our [prior]{style="color: #447099"} belief about values for the model parameters
-   $P(\theta|data)$: the [posterior]{style="color: #447099"} probability for model parameter values
:::
:::

::: aside
meme from <https://twitter.com/ChelseaParlett/status/1421291716229746689?s=20>
:::

## Likelihood

$P(data|\theta)$ : the [likelihood]{style="color: #447099"} of the data given the parameters in the model $\theta$

This part is about our [MODEL]{style="background-color: yellow; color: red"} and the parameters (aka the unknowns) in that model

> Example: a model on "time needed to run a marathon" could be a normal distribution: $y_i \backsim N(\mu, \sigma)$

So: for a certain average ($\mu$) and standard deviation ($\sigma$) we can calculate the probability of observing a marathon time of 240 minutes

## Prior

Expression of our [prior knowledge (belief)]{style="background-color: yellow; color: red"} on the parameter values

> Example: a model on "time needed to run a marathon" could be a normal distribution: $y_i \backsim N(\mu, \sigma)$

e.g., How probable is an average marathon time of 120 minutes ($P(\mu=120)$) and how probable is a standard deviation of 40 minutes ($P(\sigma=40)$)?

## Prior as a distribution

How probable are different values as average marathon time in a population?

```{r}
X <- seq(120, 300, by = 0.1)

Prob <- dnorm(
  X, 
  210,
  30
  )

data_frame(X, Prob) %>%
  ggplot(aes(x = X, y = Prob)) + 
    geom_line() +
    labs(
      x = expression(mu),
      y = "density"
    ) +
  theme_linedraw()
```

## Types of priors

::: columns
::: {.column width="50%"}
[Uninformative/Weakly informative]{style="color: #447099"}

When objectivity is crucial and you want *let the data speak for itself...*
:::

::: {.column width="50%"}
[Informative]{style="color: #447099"}

When including significant information is crucial

-   previously collected data
-   results from former research/analyses
-   data of another source
-   theoretical considerations
-   elicitation
:::
:::

## Type of priors (visually)

```{r}
X <- seq(120, 300, by = 0.1)

Weakly_Informative <- dnorm(
  X, 
  210,
  30
  )

Uninformative <- dunif(
  X,
  min = 130,
  max = 290
)


data_frame(X, Weakly_Informative, Uninformative) %>%
  pivot_longer(c(Weakly_Informative, Uninformative)) %>%
  rename(Prior = name) %>%
  ggplot(aes(x = X, y = value, lty = Prior)) + 
    geom_line() +
    labs(
      x = expression(mu),
      y = "density"
    ) +
  theme_linedraw()
```

Two potential priors for the *mean marathon time* in the population

## Bayesian theorem in stats

<br> <br> *"For Bayesians, the data are treated as fixed and the parameters vary. \[...\] Bayes' rule tells us that to calculate the posterior probability distribution we must combine a likelihood with a prior probability distribution over parameter values."* <br> (Lambert, 2018, p.88)


## Distributions, distributions and distributions

Important: 2 types/contexts

::: columns
::: {.column width="50%"}

[Distribution of **values**]{style="background-color: yellow; color: red"} of a variable (e.g., how are observed marathon times )

```{r}
Marathon_times <- seq(120, 300, by = 0.1)

Freq <- dnorm(
  Marathon_times, 
  225,
  40
  )

data.frame(Marathon_times, Freq) %>%
  ggplot(aes(x = Marathon_times, y = Freq)) + 
    geom_line() +
    labs(
      x = "Observed Marathon Times",
      y = ""
    ) +
  theme_linedraw()
```


:::

::: {.column width="50%"}

[Distribution of **probabilities**]{style="background-color: yellow; color: red"} of a parameter (prior or posterior)

```{r}
Intercept <- seq(120, 300, by = 0.1)

Probability <- dnorm(
  Marathon_times, 
  225,
  30
  )

data.frame(Intercept, Probability) %>%  
  ggplot(aes(x = Intercept, y = Probability)) + 
    geom_line() +
    labs(
      x = "Parameter Intercept",
      y = "density"
    ) +
  theme_linedraw()
```

:::
:::

This can be confusing, so each time, slow down and think!

## Let's apply this idea

Our [**Model**]{style="color: #447099"} for marathon times

$y_i \backsim N(\mu, \sigma)$

## Let's apply this idea

Our [**Priors**]{style="color: #447099"} related to mu and sigma:

```{r, echo = TRUE}
par(mfrow=c(2,2))
curve( dnorm( x , 210 , 30 ) , from=120 , to=300 ,xlab="mu", main="Prior for mu")
curve( dunif( x , 1 , 40 ) , from=-5 , to=50 ,xlab="sigma", main="Prior for sigma")
```

## Let's apply this idea

Our [**data**]{style="color: #447099"}:

```{r}
MT <- c(
  185,
  193,
  240,
  245,
  155, ## whoow; flying!
  234,
  189,
  196,
  206,
  263)
MT
```

## Let's apply this idea

What is the [**posterior probability**]{style="color: #447099"} of [mu = 180]{style="background-color: yellow; color: red"} combined with a [sigma = 15]{style="background-color: yellow; color: red"}?

```{r}
#| echo: false

options(scipen = 1000000)
```

::: columns
::: {.column width="50%"}
```{r}
#| echo: true

# Calculate the likelihood of the data
# given these parameter values of
# mu = 180 ; sigma = 15
# remember MT contains our data
Likelihood <- 
  prod(
      dnorm(
        MT ,
        mean=180 ,
        sd=15)
      )
Likelihood

```
:::

::: {.column width="50%"}
```{r}
#| echo: true

# Calculate our Prior belief

Prior <- dnorm(180, 210 , 30 ) * dunif(15 , 1 , 40 )

Prior

```
:::
:::

```{r}
#| echo: true

# Calculate posterior as product 
# of Likelihood and Prior

Product <- Likelihood * Prior

Product

```

## Let's apply this idea

What is the [**posterior probability**]{style="color: #447099"} of [mu = 210]{style="background-color: yellow; color: red"} combined with a [sigma = 30]{style="background-color: yellow; color: red"}?

::: columns
::: {.column width="50%"}
```{r}
#| echo: true

# Calculate the likelihood of the data
# given these parameter values of
# mu = 210 ; sigma = 30
# remember MT contains our data
Likelihood <- 
  prod(
      dnorm(
        MT ,
        mean=210 ,
        sd=30)
      )
Likelihood

```
:::

::: {.column width="50%"}
```{r}
#| echo: true

# Calculate our Prior belief

Prior <- dnorm(210, 210 , 30 ) * dunif(30 , 1 , 40 )

Prior

```
:::
:::

```{r}
#| echo: true

# Calculate posterior as product 
# of Likelihood and Prior

Product <- Likelihood * Prior

Product

```

## Grid approximation

We calculated the posterior probability of a combination of 2 parameters

We could repeat this systematically for following values:

```{r}
#| echo: true

# sample some values for mu and sigma
mu.list <- seq(from = 135, 
               to = 300, 
               length.out=400)
sigma.list <- seq(from = 1, 
                  to = 40, 
                  length.out = 400)

```

aka: we create a grid of possible parameter value combinations and approx the distribution of posterior probabilities

## Grid approximation applied

```{r}
post <- expand.grid(mu = mu.list, 
                    sigma = sigma.list)

post$LL <- 
  sapply(1:nrow(post), 
    function(i) sum(
      dnorm(
        MT ,
        mean=post$mu[i] ,
        sd=post$sigma[i] ,
        log=TRUE ) ) )

post$prod <- post$LL + 
  dnorm(post$mu, 210 , 30 , TRUE) + 
  dunif(post$sigma , 1 , 40 , TRUE)

# Re-scale the posterior 
# to the probability scale
post$prob <- exp(post$prod - 
                   max(post$prod)
                 )

set.seed(1975)

post %>%
  # sample 10000 rows
  # with replacement
  # higher prob higher prob to 
  # be sampled
  sample_n(size = 15000, 
           replace = TRUE, 
           weight = prob) %>%
  
  # create the plot
  ggplot(aes(x = mu, y = sigma)) +
  geom_density_2d_filled() + 
  annotate(geom="text", x=180, y=15, label="C1",
              color="red") +
  annotate(geom="text", x=210, y=30, label="C2",
              color="red") +
  scale_y_continuous(limits = c(10,40)) +
  theme_minimal()
```

C1: first combo of mu and sigma

& C2: second combo of mu and sigma

## Sampling parameter values

-   Instead of a fixed grid of combinations of parameter values,

-   sample pairs/triplets/... of parameter values

-   reconstruct the posterior probability distribution

# Why `brms`?

## Imagine

A 'simple' linear model

<br>

$$\begin{aligned}
  & MT_{i}  \sim N(\mu,\sigma_{e_{i}})\\
  & \mu = \beta_0 + \beta_1*\text{sp4week}_{i} + \beta_2*\text{km4week}_{i} + \\ 
  & \beta_3*\text{Age}_{i} + \beta_4*\text{Gender}_{i} + \beta_5*\text{CrossTraining}_{i} \\
\end{aligned}$$

<br>

So you can get a REALLY LARGE number of parameters!

## Markov Chain Monte Carlo - Why?

Complex models $\rightarrow$ Large number of parameters $\rightarrow$ exponentionally large number of combinations!

<br>

Posterior gets unsolvable by grid approximation

<br>

We will approximate the 'joint posterior' by ['smart' sampling]{style="color: #447099"}

<br>

Samples of combinations of parameter values are drawn

<br>

BUT: samples will not be random!

## MCMC - demonstrated

<br> <br> Following link brings you to an interactive tool that let's you get the basic idea behind MCMC sampling:

<br>

<https://chi-feng.github.io/mcmc-demo/app.html#HamiltonianMC,standard>

<br>

## Software

<br>

-   different dedicated software/packages are available: JAGS / BUGS / Stan

-   most powerful is [Stan]{style="color: #447099"}! Specifically the *Hamiltonian Monte Carlo* algorithm makes it the best choice at the moment

-   [Stan]{style="color: #447099"} is a probabilistic programming language that uses C++

## Example of Stan code

```{r echo = F}
load(here("Presentations", "Part1", "Model_math_naive.R"))
stancode(Model_math_naive)
```

## How `brms` works

![](brms_procedure.jpeg)

# The basics of `brms`

## `brms` syntax

Very very similar to `lm` or `lme4` and in line with typical R-style writing up of a model ...

::: columns
::: {.column width="50%"}
`lme4`

```{r}
#| echo: true
#| eval: false

Model <- lmer(
  y ~ x1 + x2 + (1|Group),
  data = Data,

  ...
)
```
:::

::: {.column width="50%"}
`brms`

```{r}
#| echo: true
#| eval: false

Model <- brm(
  y ~ x1 + x2 + (1|Group),
  data = Data,
  family = "gaussian",
  backend = "cmdstanr",
  
  ...
)
```
:::
:::

Notice:

-   `backend = "cmdstanr"` indicates the way we want to interact with Stan and C++

## Let's retake the example on running

The simplest model looked like:

$$
MT_i \sim N(\mu,\sigma_e)
$$

In `brms` this model is:

```{r echo = TRUE, eval = FALSE}
MT <- c(185, 193, 240, 245, 155, 234, 189, 196, 206, 263) # <1>

# First make a dataset from our MT vector
DataMT <- data_frame(MT)  # <2>

Mod_MT1 <- brm(                        # <3>
  MT ~ 1, # We only model an intercept # <4>
  data = DataMT,                       # <5>  
  backend = "cmdstanr",                # <6>
  seed = 1975                          # <7>
)

```

1.  create a dataset with potential marathon times
2.  change it to a data frame
3.  the brm() commando runs the model
4.  define the model
5.  define the dataset to us
6.  how brms and stan communicate
7.  make the estimation reproducible

<b> ð Try it yourself and run the model ... (don't forget to load the necessary packages: `brms` & `tidyverse`) </b>

## Basic output `brms`

```{r}
Mod_MT1<- readRDS(
  file = here("Presentations",
              "Part1",
              "Mod_MT1.RDS")
  )
```

```{r, highlight.output = c(10,14)}
#| echo: true
#| output-location: slide
summary(Mod_MT1)
```

## About chains and iterations

Markov [Chain]{style="color: red"} Monte Carlo

-   A chain of samples of parameter values
-   It is advised to run [multiple chains]{style="color: #447099"}
-   Each sample of parameter values is called an [iteration]{style="color: #447099"}
-   First X iterations are used to tune the sampler but not used to interpret the results: X burn-in iterations

`brms` defaults with **4 chains** each of **2000 iterations** of which **1000 iterations** are used as burn-in

## Changing `brms` defaults

```{r}
#| eval: false
#| echo: true
#| code-line-numbers: "5|6|7|8"
Mod_MT1 <- brm(                        
  MT ~ 1, 
  data = DataMT,   
  backend = "cmdstanr",
  chains = 5,
  iter = 6000,
  warmup = 1500,
  cores = 4,
  seed = 1975           
)
```

## Good old `plot()` function

```{r}
#| echo: true
#| output-location: slide
plot(Mod_MT1)
```

## `r fontawesome::fa("laptop-code", "white")` Your Turn {background-color="#447099" transition="slide-in"}

-   Open `MarathonData.RData`
-   Estimate your first Bayesian Models
-   Model1: only an intercept
-   Model2: introduce the effect of `km4week` and `sp4week` on `MarathonTimeM`
-   Change the `brms` defaults
-   Make plots with the `plot()` function
-   What do we learn?

## About convergence

::: columns
::: {.column width="50%"}
```{r, out.height = "99%", out.width="99%", echo = FALSE}
knitr::include_graphics("Vethari_paper.jpg")
```
:::

::: {.column width="50%"}
-   $\widehat R$ \< 1.015 for each parameter estimate

-   at least 4 chains are recommended

-   Effective Sample Size (ESS) \> 400 to rely on $\widehat R$
:::
:::

## Family time

`brms` defaults to `family = gaussian(link = "identity")`

```{r}
#| eval: false
#| echo: true
#| code-line-numbers: "3"
Mod_MT1 <- brm(                        
  MT ~ 1, 
  family = gaussian(link = "identity"),
  data = DataMT,   
  backend = "cmdstanr",
  chains = 5,
  iter = 6000,
  warmup = 1500,
  cores = 4,
  seed = 1975           
)
```

## Family time

<br>

But many alternatives are available!

<br>

The default `family` types known in `R` (e.g, `binomial(link = "logit")`, `Gamma(link = "inverse")`, `poisson(link = "log")`, ...)

<br>

see `help(family)`

<br>

```{r}
#| eval: false
#| echo: true
#| code-line-numbers: "3"
ModX <- brm(                        
  Y ~ 1, 
  family = binomial(link = "logit"),
  ...
)
```

## Family time

<br>

And even more!

<br>

`brmsfamily(...)` has a lot of possible models

<br>

see `help(brmsfamily)`

<br>

```{r}
#| eval: false
#| echo: true
#| code-line-numbers: "3"
ModX <- brm(                        
  Y ~ 1, 
  family = brmsfamily(skew_normal, 
                      link = "identity", 
                      link_sigma = "log", 
                      link_alpha = "identity"),
  ...
)
```

## Mixed effects models

<br>

`brms` can estimate models for more complex designs like `multilevel models` or more generally `mixed effects models`

<br> Random intercepts...

```{r}
#| eval: false
#| echo: true
#| code-line-numbers: "3"
ModX <- brm(                        
  Y ~ 1 + x + 
    (1 | Groupvariable), 
  ...
)
```

Random intercepts and slopes...

```{r}
#| eval: false
#| echo: true
#| code-line-numbers: "3"
ModX <- brm(                        
  Y ~ 1 + x + 
    (1 + x | Groupvariable), 
  ...
)
```

# `r fontawesome::fa("laptop-code", "white")` Homework {background-color="#447099" transition="slide-in"}
