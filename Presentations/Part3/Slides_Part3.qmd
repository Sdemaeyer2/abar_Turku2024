---
title: "Applied Bayesian Analyses in R"
subtitle: "Part 3: a mixed effects example"
author: "Sven De Maeyer"
format: 
  revealjs:
    theme: [simple, My_theme.scss]
    width: 1422
    height: 805
    slide-number: true
self-contained: true
execute: 
  echo: false
  include: true
  output: true
code:
  code-copy: true
  code-line-numbers: true
code-annotations: hover
---

```{r}
library(here)
library(tidyverse)
library(brms)
library(bayesplot)
library(ggmcmc)
library(patchwork)
library(priorsense)

load(
  file = here(
    "Presentations", 
    "WritingData.RData")
)

M3 <-readRDS(file = 
  here("Presentations",
        "Part3",
        "M3.RDS"
       )
  )

M3_custom <-readRDS(file = 
  here("Presentations",
        "Part3",
        "M3_custom.RDS"
       )
  )

M3_custom_priorsOnly <-readRDS(file = 
  here("Presentations",
        "Part3",
        "M3_custom_priorsOnly.RDS"
       )
  )
# Setting a plotting theme
theme_set(theme_linedraw() +
            theme(
              text = element_text(family = "Times", size = 14),
              panel.grid = element_blank()
              )
)

```

# Model comparison

## Leave-one-out cross-validation

Key idea:

-   leave one data point out of the data

-   re-fit the model

-   predict the value for that one data point and compare with observed value

-   re-do this *n* times

## `loo` package

Leave-one-out as described is almost impossible!

`loo` uses a *"shortcut making use of the mathematics of Bayesian inference"* [^1]

[^1]: Gelman, A., Hill, J., & Vehtari, A. (2021). *Regression and other stories.* Cambridge University Press. https://doi.org/10.1017/9781139161879

Result: ($\widehat{elpd}$): “expected log predictive density” (higher ($\widehat{elpd}$) implies better model fit without being sensitive for over-fitting!)

## `loo` code

```{r}
#| echo: true
#| eval: false

loo_Mod1 <- loo(MarathonTimes_Mod1)
loo_Mod2 <- loo(MarathonTimes_Mod2)

Comparison<- 
  loo_compare(
    loo_Mod1, 
    loo_Mod2
    )

print(Comparison, simplify = F)

#or

print(Comparison)

```

## `loo` code

```{r, out.height = "70%", out.width="70%", echo = FALSE}
knitr::include_graphics("loo_results.jpg")
```

# Bayesian Mixed Effects Model

## New example `WritingData.RData`

-   Experimental study on Writing instructions

-   2 conditions:

    -   Control condition (Business as usual)
    -   Experimental condition (Observational learning)

```{r, out.height = "75%", out.width="75%", echo = FALSE}
knitr::include_graphics("WritingDataDesc.jpg")
```

## Potential models

Model 1: Intercept varies between classes *j* + overall effect of FirstVersion

::: columns
::: {.column width="50%"}

Frequentist way of writing...

```{r, out.height = "100%", out.width="100%", echo = FALSE}
knitr::include_graphics("M1_Freq.jpg")
```

:::
::: {.column width="50%"}

Bayesian way of writing...

```{r, out.height = "100%", out.width="100%", echo = FALSE}
knitr::include_graphics("M1_Bayes.jpg")
```
:::
:::

## Potential models

Model 2: Intercept and effect of First_Version varies between classes *j* (random slopes)

::: columns
::: {.column width="50%"}

Frequentist way of writing...

```{r, out.height = "100%", out.width="100%", echo = FALSE}
knitr::include_graphics("M2_Freq.jpg")
```

:::
::: {.column width="50%"}

Bayesian way of writing...

```{r, out.height = "100%", out.width="100%", echo = FALSE}
knitr::include_graphics("M2_Bayes.jpg")
```

or 

```{r, out.height = "100%", out.width="100%", echo = FALSE}
knitr::include_graphics("M2_Bayes_vs2.jpg")
```
:::
:::

## Potential models

Model 3: M2 + Effect of condition

```{r, out.height = "100%", out.width="100%", echo = FALSE}
knitr::include_graphics("M3_Bayes.jpg")
```

## `r fontawesome::fa("laptop-code", "white")` Your Turn {background-color="#447099" transition="slide-in"}

-   Open `WritingData.RData`

-   Estimate 3 models with `SecondVersion` as dependent variable

    -   M1: fixed effect of `FirstVersion_GM` + random effect of `Class` (`(1|Class)`)
    -   M2: M1 + random effect of `FirstVersion_GM` (`(1 + FirstVersion_GM |Class)`)
    -   M3: M2 + fixed effect of `Experimental_condition`

-   Compare the models on their fit

-   What do we learn?

-   Make a summary of the best fitting model

::: aside
[Note:]{style="color: white"} `FirstVersion_GM` [is the score of the pretest centred around the mean, so a score 0 for this variable implies scoring on average for the pretest]{style="color: white"}
:::

## Divergent transitions...

-   Something to worry about!

-   Essentially: sampling of parameter estimate values went wrong

-   Fixes:

    -   sometimes fine-tuning the sampling algorithm (e.g., `control = list(adapt_delta = 0.9)`) works
    -   sometimes you need more informative priors
    -   sometimes the model is just not a good model

# Let's re-consider the priors

## Default `brms` priors

The priors of M3

```{r, out.height = "100%", out.width="100%", echo = FALSE}
knitr::include_graphics("Priors_default_M3_WritingData.jpg")
```

## Full model specification

```{r, out.height = "65%", out.width="65%", echo = FALSE}
knitr::include_graphics("M3_Bayes_withPriors.jpg")
```

## Understanding LKJcor prior

```{r}
library(ggdist)
# Plot LKJ Density
lkj_dfunc = function(K, etas=c(0.1, 0.5, 1, 1.5, 2, 5)) {
  
  etas = set_names(etas)
  cor = seq(-1,1,length=200)
  
  etas %>% 
  map_df(
    ~ dlkjcorr_marginal(cor, K=K, eta=.x)
  ) %>% 
  bind_cols(x=cor) %>% 
  pivot_longer(cols=-x, names_to='eta', values_to='density') %>% 
  ggplot(aes(x=x, y=density)) + 
    geom_area(colour=hcl(240,100,30), fill=hcl(240,100,65)) +
    facet_wrap(vars(eta)) +
    scale_y_continuous(expand=expansion(c(0,0.05))) +
    coord_cartesian(ylim=c(0,2)) +
    labs(title=paste0(ifelse(K==2, "", "Marginal "),
                      "LKJ correlation distribution for number of correlations =", K,
                      " and various values of eta"),
         x="Correlation") 
}

lkj_dfunc(2)
```

## Understanding the priors for the "fixed effects"

The overall intercept

```{r}
library(metRology)

Prior_mu <- ggplot( ) +
  stat_function(
    fun = dt.scaled,    # We use the dt.scaled function of metRology
    args = list(df = 3, mean = 110.4, sd = 13.3), # 
    xlim = c(80,140)
  ) +
  scale_y_continuous(name = "density") +
  labs(title = "Prior for the intercept",
       subtitle = "student_t(3,110.4,13.3)")

Prior_mu 
```

Maybe a little wide?? But, let's stick with it...

## Understanding the priors for the "fixed effects"

- What about the overall effect of *FirstVersion_GM*? 

- What about the overall effect of *ExperimentalCondition*?

Nice to know:

```{r}
#| message: false
#| warning: false
#| error: false
#| echo: true
#| eval: true
sd(WritingData$SecondVersion)
```

## Understanding the priors for the "fixed effects"

What about the overall effect of *FirstVersion_GM*? 

<br>
<br>

::: columns
::: {.column width="60%"}

```{r}
#| message: false
#| warning: false
#| error: false
#| echo: true
#| eval: false
# Normal distribution with mean = 1 and sd = 5
Prior_beta1 <- ggplot( ) +
  stat_function(
    fun = dnorm,    # We use the normal distribution
    args = list(mean = 1, sd = 5), # 
    xlim = c(-9,11)
  ) +
  scale_y_continuous(name = "density") +
  labs(title = "Prior for the effect of FirstVersion",
       subtitle = "N(1,5)") +
  geom_vline(xintercept=1,linetype=3)

Prior_beta1 
```

:::
::: {.column width="40%"}
```{r}
#| message: false
#| warning: false
#| error: false
#| echo: false
#| eval: true

# Normal distribution with mean = 1 and sd = 5
Prior_beta1 <- ggplot( ) +
  stat_function(
    fun = dnorm,    # We use the normal distribution
    args = list(mean = 1, sd = 5), # 
    xlim = c(-9,11)
  ) +
  scale_y_continuous(name = "density") +
  labs(title = "Prior for the effect of FirstVersion",
       subtitle = "N(1,5)")+
  geom_vline(xintercept=1,linetype=3)

Prior_beta1 
```

:::
:::

## Understanding the priors for the "fixed effects"

What about the overall effect of *ExperimentalCondtion*? 

Assuming a "small effect size" $=$ 0.2 SD's $=$ Effect of 3.4 points on our scale

We also want to give probability to big effects as well as negative effects, so we set the *standard deviation of our prior distribution* wide enough (e.g., 17).

::: columns
::: {.column width="60%"}

```{r}
#| message: false
#| warning: false
#| error: false
#| echo: true
#| eval: false
# Normal distribution with mean = 3.4 and sd = 17
Prior_beta2 <- ggplot( ) +
  stat_function(
    fun = dnorm,    # We use the normal distribution
    args = list(mean = 3.4, sd = 17), # 
    xlim = c(-30.6,37.4)
  ) +
  scale_y_continuous(name = "density") +
  labs(title = "Prior for the effect of FirstVersion",
       subtitle = "N(3.4,17)") +
  geom_vline(xintercept=3.4,linetype=3)

Prior_beta2 
```

:::
::: {.column width="40%"}
```{r}
#| message: false
#| warning: false
#| error: false
#| echo: false
#| eval: true
# Normal distribution with mean = 3.4 and sd = 17
Prior_beta2 <- ggplot( ) +
  stat_function(
    fun = dnorm,    # We use the normal distribution
    args = list(mean = 3.4, sd = 17), # 
    xlim = c(-30.6,37.4)
  ) +
  scale_y_continuous(name = "density") +
  labs(title = "Prior for the effect of FirstVersion",
       subtitle = "N(3.4,17)") +
  geom_vline(xintercept=3.4,linetype=3)

Prior_beta2 
```

:::
:::

## Understanding the variances between Classes!

Variance between classes **for the intercept** (expressed as an SD)? 

How large are differences between classes for an average student (for first version)?

```{r}
Prior_sd_mu <- ggplot( ) +
  stat_function(
    fun = dt.scaled,    # We use the dt.scaled function of metRology
    args = list(df = 3, mean = 0, sd = 13.3), # 
    xlim = c(0,40)
  ) +
  scale_y_continuous(name = "density") +
  labs(title = "Prior for the SD expressing differences between classes in intercept",
       subtitle = "student_t(3, 0, 13.3)") +
  geom_vline(xintercept=13.3,linetype=3) +
  geom_vline(xintercept=26.6,linetype=3)

Prior_sd_mu 
```

So, a high probability for SD's bigger than 13.3, what does that mean?

## Understanding the variances between Classes!

Imagine an **SD of 13.3** for the differences between classes' intercepts


```{r}
#| message: false
#| warning: false
#| error: false
#| echo: false
#| eval: true

Difference_classes <- ggplot( ) +
  stat_function(
    fun = dnorm,    # We use the normal distribution
    args = list(mean = 110.4, sd = 13.3), # 
    xlim = c(80,140)
  ) +
  scale_y_continuous(name = "density") +
  labs(title = "Hypothetical distribution of CLASS AVERAGES",
       subtitle = "N(110.4, 13.3)") +
  geom_vline(xintercept=110.4,linetype=3) +
  geom_vline(xintercept=134,linetype=3)+
  geom_vline(xintercept=83.8,linetype=3)

Difference_classes 
```

95% of classes will score between 83.8 and 137

[Distribution alert!! This is not a probability distribution for a parameter but a distribution of hypothetical observations (classes)!!]{style="background-color: yellow; color: red"}

## Understanding the variances between Classes!

Imagine an **SD of 20** for the differences between classes' intercepts

```{r}
#| message: false
#| warning: false
#| error: false
#| echo: false
#| eval: true

Difference_classes <- ggplot( ) +
  stat_function(
    fun = dnorm,    # We use the normal distribution
    args = list(mean = 110.4, sd = 20), # 
    xlim = c(60,160)
  ) +
  scale_y_continuous(name = "density") +
  labs(title = "Hypothetical distribution of CLASS AVERAGES",
       subtitle = "N(110.4, 20)") +
  geom_vline(xintercept=110.4,linetype=3) +
  geom_vline(xintercept=150.4,linetype=3)+
  geom_vline(xintercept=79.4,linetype=3)

Difference_classes 
```

95% of classes will score between 70.4 and 150.4

[Distribution alert!! This is not a probability distribution for a parameter but a distribution of hypothetical observations (classes)!!]{style="background-color: yellow; color: red"}

## Understanding the variances between Classes!

Variance between classes **for the effect of FirstVersion** (expressed as an SD)? 

How large are differences between classes for the effect of first version?

```{r}
Prior_sd_slopes <- ggplot( ) +
  stat_function(
    fun = dt.scaled,    # We use the dt.scaled function of metRology
    args = list(df = 3, mean = 0, sd = 13.3), # 
    xlim = c(0,40)
  ) +
  scale_y_continuous(name = "density") +
  labs(title = "Prior for the SD expressing differences between classes in slopes",
       subtitle = "student_t(3, 0, 13.3)") +
  geom_vline(xintercept=13.3,linetype=3) +
  geom_vline(xintercept=26.6,linetype=3)

Prior_sd_slopes 
```

So, a high probability for SD's bigger than 13.3, what does that mean?

## Understanding the variances between Classes!

Imagine an **SD of 13.3** for the differences in slopes between classes

```{r}
#| message: false
#| warning: false
#| error: false
#| echo: false
#| eval: true

Difference_classes <- ggplot( ) +
  stat_function(
    fun = dnorm,    # We use the normal distribution
    args = list(mean = 1, sd = 13.3), # 
    xlim = c(-28,30)
  ) +
  scale_y_continuous(name = "density") +
  labs(title = "Hypothetical distribution of CLASS AVERAGES",
       subtitle = "N(110.4, 13.3)") +
  geom_vline(xintercept=1,linetype=3) +
  geom_vline(xintercept=-25.6,linetype=3)+
  geom_vline(xintercept=27.6,linetype=3)

Difference_classes 
```

For 95% of classes the effect of first version will vary between -25.6 and 27.6!

[Distribution alert!! This is not a probability distribution for a parameter but a distribution of hypothetical observations (classes)!!]{style="background-color: yellow; color: red"}

## Model 3 with custom priors

```{r, out.height = "65%", out.width="65%", echo = FALSE}
knitr::include_graphics("M3_Bayes_customPriors.jpg")
```

## Set custom priors in `brms`

```{r}
#| message: false
#| warning: false
#| error: false
#| echo: true
#| eval: false
Custom_priors <- 
  c(
    set_prior(
      "normal(1,5)", 
      class = "b", 
      coef = "FirstVersion_GM"),
    set_prior(
      "normal(3.4,17)", 
      class = "b", 
      coef = "Experimental_condition"),
    set_prior(
      "student_t(3,0,5)", 
      class = "sd", 
      coef = "FirstVersion_GM",
      group = "Class")
  )
```

## Estimate the model with custom priors

```{r}
#| message: false
#| warning: false
#| error: false
#| echo: true
#| eval: false
#| code-line-numbers: "4"
M3_custom <- brm(
  SecondVersion ~ FirstVersion_GM + Experimental_condition + (1 + FirstVersion_GM |Class),
  data = WritingData,
  prior = Custom_priors,
  backend = "cmdstanr",
  cores = 4,
  seed = 1975
)
```

# Prior Predictive Check

## Put priors in the context of the likelihood

<br>

Did you set sensible priors? How informative are the priors (taken together)?

<br>

-   Simulate data based on the model and the priors

<br>

-   Visualize the simulated data and compare with real data

<br>

-   Check if the plot shows impossible simulated datasets



## In `brms`

<br>

Step 1: Fit the model with custom priors with option `sample_prior="only"`

<br>

```{r}
#| message: false
#| warning: false
#| error: false
#| echo: true
#| eval: false
#| cache: true
#| code-line-numbers: "5|9"
M3_custom_priorsOnly <- 
  brm(
    SecondVersion ~ FirstVersion_GM + Experimental_condition + (1 + FirstVersion_GM |Class),
    data = WritingData,
    prior = Custom_priors,
    backend = "cmdstanr",
    cores = 4,
    seed = 1975, 
    sample_prior = "only"
)
```

::: {.callout-note}
This will not work with the default priors of `brms` as `brms`sets flat priors for beta's. So, you have to set custom priors for the effects of independent variables to be able to perform the prior predictive checks.
:::

## In `brms`

<br>

Step 2: visualize the data with the `pp_check( )` function

<br>

```{r}
#| echo: true
#| eval: false

set.seed(1975)

pp_check(
  M3_custom_priorsOnly, 
  ndraws = 300) # number of simulated datasets you wish for

```

## In `brms`

300 distributions of simulated datasets (scores on `SecondVersion`) overlayed by our observed data (to have an anchoring point)

```{r}
#| echo: false

set.seed(1975)

pp_check(
  M3_custom_priorsOnly, 
  ndraws = 300) # number of simulated datasets you wish for

```

:::aside
*Notice how we get datasets that contain a proportion of scores lower than 0 !!*
:::

## Check some summary statistics

-   How are summary statistics of simulated datasets (e.g., median, min, max, ...) distributed over the datasets?

-   How does that compare to our real data?

-   Use `type = "stat"` argument within `pp_check()`

```{r}
#| echo: true
#| eval: false

pp_check(M3_custom_priorsOnly, 
         type = "stat", 
         stat = "median")
```

## Check some summary statistics

300 medians of simulated datasets (scores on `SecondVersion`) overlayed by the median of our observed data (to have an anchoring point)

```{r}
#| echo: false
#| eval: true

pp_check(M3_custom_priorsOnly, 
         type = "stat", 
         stat = "median")
```

:::aside
*Notice that some simulated datasets would imply a very low (e.g., lower than 60) or very high (e.g., higher than 150) median score on `SecondVersion`*
:::

## Check some summary statistics

-   Let's check the minima of all simulated datasets based on our priors

```{r}
#| echo: true
#| eval: false

pp_check(M3_custom_priorsOnly, 
         type = "stat", 
         stat = "min")
```

## Check some summary statistics

300 lowest scores (in the simulated datasets) overlayed by the lowest observed score in our data (to have an anchoring point)

```{r}
#| echo: false
#| eval: true

pp_check(M3_custom_priorsOnly, 
         type = "stat", 
         stat = "min")
```

:::aside
*Notice that the majority of simulated datasets would imply a low and sometimes even impossible lowest score for our dependent variable (e.g., lower than 0)*
:::

## Check our grouped data (we have classes)

For each datapoint, situated within it's group (Class) a set of 300 predicted scores based on the prior only.


```{r}
#| echo: true
#| fig-width: 13.5
#| fig-height: 7.5
#| output-location: slide

pp_check(
  M3_custom_priorsOnly ,
  type = "intervals_grouped",
  group = "Class",
  ndraws = 300)
```


## Possible conclusion

*Put priors in the context of the likelihood*

<br>


The priors are non-informative!! 

<br>

They do not predict our data perfectly. 

# Prior sensitivity analyses

## Why prior sensitivity analyses?

-   Often we rely on 'arbitrary' chosen (default) weakly informative priors

-   What is the influence of the prior (and the likelihood) on our results?

-   You could ad hoc set new priors and re-run the analyses and compare (a lot of work, without strict sytematical guidelines)

-   Semi-automated checks can be done with `priorsense` package

## Using the `priorsense` package

Recently a package dedicated to prior sensitivity analyses is launched

```{r}
#| eval: false
#| echo: true
# install.packages("remotes")
remotes::install_github("n-kall/priorsense")
```

Key-idea: power-scaling (both prior and likelihood)

background reading:

-   <https://arxiv.org/pdf/2107.14054.pdf>

YouTube talk:

-   <https://www.youtube.com/watch?v=TBXD3HjcIps&t=920s>

## Basic table with indices

First check is done by using the `powerscale_sensitivity( )` function

-   column prior contains info on sensitivity for prior (should be lower than 0.05)

-   column likelihood contains info on sensitivity for likelihood (that we want to be high, 'let our data speak')

-   column diagnosis is a verbalization of potential problem (- if none)

```{r}
#| echo: true
#| output-location: slide
powerscale_sensitivity(M3_custom)
```

## Visualization of prior sensitivity

```{r}
#| echo: true
#| warning: false
#| message: false
#| fig-width: 13.5
#| fig-height: 7.5
#| cache: true
#| output-location: slide

powerscale_plot_dens(
  powerscale_sequence(
    M3_custom
    ),
  variables = c(
      "b_Intercept",
      "b_FirstVersion_GM",
      "b_Experimental_condition"
    )
  )
```

## Visualization of prior sensitivity

```{r}
#| echo: true
#| warning: false
#| message: false
#| fig-width: 13.5
#| fig-height: 7.5
#| cache: true
#| output-location: slide

powerscale_plot_dens(
  powerscale_sequence(
    M3_custom
    ),
  variables = c(
      "sd_Class__Intercept",
      "sd_Class__FirstVersion_GM"
    )
  )
```

## Visualization of prior sensitivity

```{r}
#| echo: true
#| warning: false
#| message: false
#| cache: true
#| output-location: slide

powerscale_plot_quantities(
  powerscale_sequence(
    M3_custom
    ),
  variables = c(
      "b_Experimental_condition"
      )
  )
```

