---
title: "Applied Bayesian Analyses in R"
subtitle: "Part 3: time to mix"
author: "Sven De Maeyer"
format: 
  revealjs:
    theme: [simple, My_theme.scss]
    width: 1422
    height: 805
    slide-number: true
editor: visual
self-contained: true
execute: 
  echo: false
  include: true
  output: true
code:
  code-copy: true
  code-line-numbers: true
code-annotations: hover
---

```{r}
library(here)
library(tidyverse)
library(brms)
library(bayesplot)
library(ggmcmc)
library(patchwork)
library(priorsense)

load(
  file = here(
    "Presentations", 
    "WritingData.RData")
)

M3 <-readRDS(file = 
  here("Presentations",
        "Part3",
        "M3.RDS"
       )
  )

# Setting a plotting theme
theme_set(theme_linedraw() +
            theme(
              text = element_text(family = "Times", size = 14),
              panel.grid = element_blank()
              )
)

```

# Model comparison

## Leave-one-out cross-validation

Key idea:

-   leave one data point out of the data

-   re-fit the model

-   predict the value for that one data point and compare with observed value

-   re-do this *n* times

## `loo` package

Leave-one-out as described is almost impossible!

`loo` uses a *"shortcut making use of the mathematics of Bayesian inference"* [^1]

[^1]: Gelman, A., Hill, J., & Vehtari, A. (2021). *Regression and other stories.* Cambridge University Press. https://doi.org/10.1017/9781139161879

Result: (\widehat{elpd}): “expected log predictive density” (higher (\widehat{elpd}) implies better model fit without being sensitive for over-fitting!)


## `loo` code

```{r}
#| echo: true
#| eval: false

loo_Mod1 <- loo(MarathonTimes_Mod1)
loo_Mod2 <- loo(MarathonTimes_Mod2)

Comparison<- 
  loo_compare(
    loo_Mod1, 
    loo_Mod2
    )

print(Comparison)
```

## `loo` code



# Bayesian Mixed Effects Model


## New example data `WritingData.RData`

-   Experimental study on Writing instructions

-   2 conditions:

    -   Control condition (Business as usual)
    -   Experimental condition (Observational learning)

```{r, out.height = "50%", out.width="50%", echo = FALSE}
knitr::include_graphics("WritingDataDesc.jpg")
```

## `r fontawesome::fa("laptop-code", "white")` Your Turn {background-color="#447099" transition="slide-in"}

-   Open `WritingData.RData`

-   Estimate 3 models with `SecondVersion` as dependent variable

    -   M1: fixed effect of `FirstVersion_GM` + random effect of `Class` (`(1|Class)`)
    -   M2: M1 + random effect of `FirstVersion_GM` (`(1 + FirstVersion_GM |Class)`)
    -   M3: M2 + fixed effect of `Experimental_condition`

-   Compare the models on their fit

-   What do we learn?

-   Make a summary of the best fitting model

::: aside
[Note:]{style="color: white"} `FirstVersion_GM`[is the score of the pretest centred around the mean, so a score 0 for this variable implies scoring on average for the pretest]{style="color: white"}
:::

## Divergent transitions...

-   Something to worry about!

-   Essentially: sampling of parameter estimate values went wrong

-   Fixes:

    -   sometimes fine-tuning the sampling algorithm (e.g., `control = list(adapt_delta = 0.9)`) works
    -   sometimes you need more informative priors
    -   sometimes the model is just not a good model


## Let's re-consider the priors



## Questions?

<br>

Do not hesitate to contact me!

<br>

[sven.demaeyer\@uantwerpen.be](mailto:sven.demaeyer@uantwerpen.be){.email}

# `r fontawesome::fa("thumbs-up", "white")` THANK YOU! {background-color="#447099" transition="slide-in"}
